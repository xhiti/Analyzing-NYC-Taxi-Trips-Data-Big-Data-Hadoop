--Hive Practice
create external table employee (
  id string,
  fname string,
  lname string,
  department_id string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/ user / 
sample_data / employee /';


create external table department (
  id string,
  dept_name string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/ user / 
sample_data / department /';

--External Table in Hive
create external table salary (
  salary_id string,
  employee_id string,
  payment double,
  date string 
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/ user / 
sample_data / salary /';

--Internal Table
create table salary_internal as 
select salary_id, employee_id, payment, date from salary;

--File Formats in Hive:
--Text File:Linux Applications 2021 MSc Big Data Hadoop
create table salary_text (
  salary_id string,
  employee_id string,
  payment double,
  date string 
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' stored as textfile;
INSERT OVERWRITE TABLE salary_text SELECT * FROM salary;

--RCFile:
create table salary_rcfile (
  salary_id string,
  employee_id string,
  payment double,
  date string 
) stored as rcfile;
SET hive.exec.compress.output = true;
SET mapred.output.compression.type = BLOCK;
SET mapred.output.compression.codec = org.apache.hadoop.io.compress.SnappyCodec;
INSERT OVERWRITE TABLE salary_rcfile SELECT * FROM salary;

--Parquet:
create table salary_parquetfile (
  salary_id string,
  employee_id string,
  payment double,
  date string 
) stored as parquetfile;
INSERT OVERWRITE TABLE salary_parquetfile SELECT * FROM salary;

--ORC:
create table salary_orc (
  salary_id string,
  employee_id string,
  payment double,
  date string
) stored as orc;Linux Applications 2021 MSc
INSERT OVERWRITE TABLE salary_orc SELECT * FROM salary;

--finds the number of employees per department with a salary higher than 1000 in August 2008.
select d.dept_name, count (e.id) as employee_count
from department d
join employee e on d.id = e.department_id
join salary s on s.employee_id = e.id
where s.payment> 1000 and month (s.date) = 8 and year (s.date) = 2008
group by d.dept_name;

--from commandline
hive -e 'select * from hive_demo.salary limit 2'
hive -S -e 'select * from hive_demo.salary_orc limit 5'




--Practical Pig
--load with schema
e = LOAD '/user/sample_data/employee/employee.csv' USING PigStorage (',') 
as (eid: chararray, fname: chararray, lname: chararray, department: chararray);
e_sample = limit e 10;
dump e_sample;Linux Applications 2021 MSc


--load without schema
e1 = LOAD '/user/sample_data/employee/employee.csv' USING PigStorage (',');
e1_sample = limit 10 e1;
dump e1;

-- a simple query query1
d = LOAD '/user/sample_data/salary/salary.csv' USING PigStorage (',') as 
(salary_id: chararray, employ_id: chararray, payment: double, p_date: datetime);
d_sample = limit d 10;
dump d_sample;

--a simple query query2
g = LOAD '/user/sample_data/employee/employee.csv' USING PigStorage (',') as 
(eid: chararray, fname: chararray, lname: chararray, department: chararray);
f = group g by department;
f_sample = limit f 2;
dump f_sample;

-- JOIN in Pig
emp = LOAD '/user/sample_data/employee/employee.csv' USING PigStorage (',') 
as (emp_id: chararray, fname: chararray, lname: chararray, dept_id: chararray);
dep = LOAD '/user/sample_data/department/department.csv' USING PigStorage 
(',') as (dept_id: chararray, dept_name: chararray);
dep_emp_join = JOIN emp by (dept_id), dep by (dept_id);
grouped = group dep_emp_join by dept_name;
group_by_count = foreach grouped {
  uniq_emp = DISTINCT dep_emp_join.emp_id;
  generate group, COUNT (uniq_emp) as emp_count;
};
dump group_by_count;

--store pig results
emp = LOAD '/user/sample_data/employee/employee.csv' USING PigStorage (',') 
as (emp_id: chararray, fname: chararray, lname: chararray, dept_id: chararray);
dep = LOAD '/user/sample_data/department/department.csv' USING PigStorage 
(',') as (dept_id: chararray, dept_name: chararray);
dep_emp_join = JOIN emp by (dept_id), dep by (dept_id);
grouped = group dep_emp_join by dept_name;
group_by_count = foreach grouped {
  uniq_emp = DISTINCT dep_emp_join.emp_id;
  generate group, COUNT (uniq_emp) as emp_count;
};
store group_by_count into '/ user / sample_data / pig_output';




